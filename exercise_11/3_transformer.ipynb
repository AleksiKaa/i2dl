{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "026e0255",
   "metadata": {},
   "source": [
    "# Transformers - Attention is All You Need\n",
    "Welcome to the captivating world of Transformers in Natural Language Processing (NLP)! In recent years, Transformers have emerged as a groundbreaking paradigm in NLP, revolutionizing how we comprehend and process language. Unlike older architectures such as LSTMs (Long Short-Term Memory) and CNNs (Convolutional Neural Networks), Transformers leverage self-attention mechanisms, enabling them to capture long-range dependencies more effectively and allowing for parallelization of computations. The seminal paper ['Attention Is All You Need'](https://arxiv.org/abs/1706.03762) introduced the Transformer model, showcasing its prowess in various language tasks and laying the foundation for its widespread adoption. This Jupyter notebook is designed to unravel the intricacies of Transformers, highlighting their architectural nuances, contrasting them with traditional models, and demonstrating their applications across diverse NLP domains! (And in case you were wondering - yes, a certain transformer model created this text :D)\n",
    "\n",
    "We will stick very closely to the implementation presented in the paper to build our own translation model! There is no need to read it though, you can find the necessary sections of the paper before each task, together with some explanations. \n",
    "\n",
    "Alright - let's do this!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3c01bc",
   "metadata": {},
   "source": [
    "## (Optional) Mount folder in Colab\n",
    "\n",
    "Uncomment the following cell to mount your gdrive if you are using the notebook in google colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e6fa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the following lines if you want to use Google Colab\n",
    "# We presume you created a folder \"i2dl\" within your main drive folder, and put the exercise there.\n",
    "# NOTE: terminate all other colab sessions that use GPU!\n",
    "# NOTE 2: Make sure the correct exercise folder (e.g exercise_11) is given.\n",
    "\n",
    "\"\"\"\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "gdrive_path='/content/gdrive/MyDrive/i2dl/exercise_11'\n",
    "\n",
    "# This will mount your google drive under 'MyDrive'\n",
    "drive.mount('/content/gdrive', force_remount=True)\n",
    "# In order to access the files in this notebook we have to navigate to the correct folder\n",
    "os.chdir(gdrive_path)\n",
    "# Check manually if all files are present\n",
    "print(sorted(os.listdir()))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f568e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.util import notebook_util as util\n",
    "from exercise_code.network import *\n",
    "from exercise_code.tests import *\n",
    "from exercise_code.trainer import MPS_AVAILABLE\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "root_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "model_path = os.path.join(os.getcwd(), 'models')\n",
    "submission_path = os.path.join(os.getcwd(), 'submission_files')\n",
    "pretrained_model_path = os.path.join(model_path, 'pretrainedModels')\n",
    "dataset_path = os.path.join(root_path, 'datasets', 'transformerDatasets')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a27adc",
   "metadata": {},
   "source": [
    "## Model Overview\n",
    "\n",
    "<!-- <img src=\"images/Transformer-Transformer.drawio.png\" width=\"2500\"> -->\n",
    "<img src=\"https://i2dl.vc.in.tum.de/static/images/Transformer-Transformer.drawio.png\"  width=\"2500\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The model consists of two bigger building blocks, the encoder and the decoder. The encoder processes the input to our model. In our translation model, this input would be the sentence in the source language. The decoder part iteratively produces an output sequence. As an input it takes the already predicted words and given that sequence and the encoder input, it produces an output sequence as follows:\n",
    "\n",
    "| Encoder Input                       | Decoder Input                                      | Decoder Output                                   |\n",
    "|-------------------------------------|----------------------------------------------------|--------------------------------------------------|\n",
    "| [\"Hello\" ,\"how\", \"are\", \"you\", \"?\"] | [\"\\<start>\"]                                       | [\"Hallo\"]                                        |\n",
    "| [\"Hello\" ,\"how\", \"are\", \"you\", \"?\"] | [\"\\<start>\", \"Hallo\"]                              | [\"Hallo\", \"wie\"]                                 |\n",
    "| [\"Hello\" ,\"how\", \"are\", \"you\", \"?\"] | [\"\\<start>\", \"Hallo\", \"wie\"]                       | [\"Hallo\", \"wie\", \"geht's\"]                       |\n",
    "| [\"Hello\" ,\"how\", \"are\", \"you\", \"?\"] | [\"\\<start>\", \"Hallo\", \"wie\", \"geht's\"]             | [\"Hallo\", \"wie\", \"geht's\", \"dir\"]                |\n",
    "| [\"Hello\" ,\"how\", \"are\", \"you\", \"?\"] | [\"\\<start>\", \"Hallo\", \"wie\", \"geht's\", \"dir\"]      | [\"Hallo\", \"wie\", \"geht's\", \"dir\", \"?\"]           |\n",
    "| [\"Hello\" ,\"how\", \"are\", \"you\", \"?\"] | [\"\\<start>\", \"Hallo\", \"wie\", \"geht's\", \"dir\", \"?\"] | [\"Hallo\", \"wie\", \"geht's\", \"dir\", \"?\", \"\\<end>\"] |\n",
    "\n",
    "And the predicted end token breaks the loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d585523a",
   "metadata": {},
   "source": [
    "# Embedding Layer\n",
    "\n",
    "Let's discuss the input to our model first: So far, with images, its quite clear that similar values on a pixel level correspond to somewhat similar images. Now, how about text?\n",
    "\n",
    "As discussed in the tokenizer notebook, our words or sub-words that make up a sentence are transformed into individual IDs.\n",
    "Imagine what would happen if we used these token IDs directly as an input to the neural network: Each input neuron would be activated proportionally to the ID of the token. This would mean that the model would assume that the tokens with similar IDs are somehow more related and that the distance between the token IDs is meaningful. This is, of course, not the case! \"House\" might have ID 627 and \"Bungalow\" might be ID 9384, even though they mean similar things and a classifier should be able to classify both words as buildings. Remember that during the training of the tokenizer, we didn't care much about the words semantics, only about the frequency!\n",
    "\n",
    "Ideally, we would like to represent our words in a way that similar meanings have similar values. This is where **embeddings** come in:\n",
    "Embeddings are vector representations of our tokens, so that tokens with similar meanings have similar vectors, i.e. closer together in the vector space (called **embedding space**).\n",
    "\n",
    "Let's look at some simple examples! First, the tokens with their raw IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0660fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, embeddings = util.create_embeddings(1, token_id=True)\n",
    "util.plot_embeddings(labels, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166e8774",
   "metadata": {},
   "source": [
    "So the first idea you might have is to reorder the IDs and to group similar words together, assuming we could somehow \"measure\" similarity between two words! Let's have a look at what this could look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6292766",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, embeddings = util.create_embeddings(1)\n",
    "util.plot_embeddings(labels, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a392c301",
   "metadata": {},
   "source": [
    "Even with this relatively simple algorithm, we can already see certain groups form. On the right we have school related words, on the left we have animals. \n",
    "\n",
    "What we have actually just created are basically 1D Embeddings!\n",
    "\n",
    "However, with 1D Embeddings we can't really encode more complicated relationships, like for example a triangle relationship, where three words are all equally similar to each other. By adding dimensions to our embedding, we can see more complex connections between word pairs. In 2D this could look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad1fc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, embeddings = util.create_embeddings(2)\n",
    "util.plot_embeddings(labels, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09521599",
   "metadata": {},
   "source": [
    "In our transformer model, we will actually create an embedding with 512 dimensions! And we won't even have to define a similarity function between words, during training the model will decide on its own where to place words in the embedding space!\n",
    "\n",
    "At the end of the day, the embedding layer is a simple lookup table that maps each token ID to these embedding vectors.\n",
    "\n",
    "If you are more interested in Embeddings, go check out this awesome tool, that let's you play around with word embeddings: http://vectors.nlpl.eu/explore/embeddings/en/\n",
    "\n",
    "Also, Computerphile made a nice video on vector embeddings, you can find it here: https://www.youtube.com/watch?v=gQddtTdmG_8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4310bd92",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>WARNING: Task Implementation</h3>\n",
    "    <p>Throughout this notebook you will as always have to complete several tasks to complete the individual modules! Please be aware though, that some Modules may have several tasks that have to be completed, but please <strong>only</strong> concentrate on the <strong>current task</strong> and the <strong>corresponding hints</strong> (if there are any;). In other words, if you are working on Task 1, and there is also Task 4 in the TODOs of that Module, you don't have to work on it at this moment! <br>\n",
    "    Also, if we mention any specific pytorch modules in the task description or hints, you <strong>are allowed to use them!</strong> With that said, let's work on your first task!\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b214995c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 1: Implement</h3>\n",
    "    <p>Implement the class <code>Embedding</code> in <code>exercise_code/network/embedding.py</code>.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0450c30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = test_task_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d15b318",
   "metadata": {},
   "source": [
    "We will be adding another feature to the embedding layer later in the notebook, however to understand the need for it we have to explain the attention mechanism first! </br>\n",
    "So far, our Embedding Layer looks somewhat like this: \n",
    "\n",
    "<!-- <img src=\"images/Transformer-Embedding.drawio.png\" width=\"1000\"> -->\n",
    "<img src=\"https://i2dl.vc.in.tum.de/static/images/Transformer-Embedding.drawio.png\" width=\"1000\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c884d3c8",
   "metadata": {},
   "source": [
    "## Attention Mechanism\n",
    "\n",
    "### Improving Noisy Measurements\n",
    "Imagine you have taken some measurement x(t), but notice that it is quite noisy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f386ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "measurement_data = util.get_measurement_data()\n",
    "plt.plot(measurement_data['time'], measurement_data['data_noise'])\n",
    "plt.xlabel('Time'), plt.ylabel('Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffa4163",
   "metadata": {},
   "source": [
    "So a way we could try to improve these measurements is by averaging out the values using a weighted average:\n",
    "\n",
    "\n",
    "$\\tilde{x}_i = \\sum_{j=1}^N s_{ij} x_j$\n",
    "\n",
    "We could base the weights on a similarity score between the time stamps. Values that are closer together time-wise, should have a larger impact/weight compared to values that are further apart!\n",
    "\n",
    "$s_{ij} = sim(t_i, t_j)$\n",
    "\n",
    "A possible option as a similarity function is the squared exponential kernel:\n",
    "\n",
    "$sim(t_i, t_j) = \\exp\\left(-\\frac{(t_i - t_j)^2}{\\sigma ^2}\\right)$\n",
    "\n",
    "Let's have a look at the similarity function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415831be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the squared exponential kernel \n",
    "def similarity(x1, x2):\n",
    "    sigma = 1\n",
    "    return np.exp(-((x1 - x2)**2 / (sigma ** 2)))\n",
    "\n",
    "# Define the time stamps\n",
    "time_stamps = np.linspace(-10, 10, 1000)\n",
    "\n",
    "# Compute the similarity scores between 0 and all other time stamps\n",
    "scores = similarity(0, time_stamps)\n",
    "\n",
    "# Plot the similarity scores\n",
    "plt.plot(time_stamps, scores)\n",
    "plt.title('Similarity Function')\n",
    "plt.xlabel('Time'), plt.ylabel('Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9141f8c2dd8712f2",
   "metadata": {},
   "source": [
    "As you can see, the closer we get to the zero, the higher the score gets!\n",
    "And now let's plot the actual score matrix!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc964ee8068ce519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the actual timestamps from the measurement data\n",
    "time_stamps = measurement_data['time']\n",
    "\n",
    "# Compute the similarity scores between all time stamps\n",
    "scores = similarity(time_stamps[:, None], time_stamps[None, :])\n",
    "\n",
    "# Plot the similarity scores\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(scores, interpolation='nearest')\n",
    "plt.title('Score Matrix')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Time Stamp ID'), plt.ylabel('Time Stamp ID')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9582c9b",
   "metadata": {},
   "source": [
    "What you should see a high value along the diagonal, and the values decrease the further you move away from the diagonal. \n",
    "\n",
    "Next, let's compute these averages! Note, that the average can also be described as a Matrix-Vector product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf4ea86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute filtered data\n",
    "filter_data = scores @ measurement_data['data_noise']\n",
    "\n",
    "# Plot the filtered data\n",
    "plt.plot(time_stamps, filter_data)\n",
    "plt.xlabel('Time'), plt.ylabel('Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631cc387",
   "metadata": {},
   "source": [
    "Looks a lot better! However, we are actually filtering out a lot of the underlying signal. Try changing sigma in the next cell to a different values (e.g $\\sigma = 0.5$ or even $\\sigma = 0.1$) and have a look at the function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5125b548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit Sigma here\n",
    "sigma = 0.5\n",
    "\n",
    "def similarity(x1, x2):\n",
    "    return np.exp(-((x1 - x2)**2 / (sigma ** 2)))\n",
    "\n",
    "# Compute the similarity scores between all time stamps\n",
    "scores = similarity(time_stamps[:, None], time_stamps[None, :])\n",
    "\n",
    "# Plot the similarity scores\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(scores, interpolation='nearest')\n",
    "plt.title('Score Matrix')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Time Stamp ID'), plt.ylabel('Time Stamp ID')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9b1acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute filtered data\n",
    "filter_data = scores @ measurement_data['data_noise']\n",
    "\n",
    "# Plot the filtered data\n",
    "plt.plot(time_stamps, filter_data)\n",
    "plt.xlabel('Time'), plt.ylabel('Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e418d1",
   "metadata": {},
   "source": [
    "If you did it then - Nice! We are not loosing that much information anymore! If not - Come on, it's like only one line ;P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb6c816",
   "metadata": {},
   "source": [
    "We still have one problem in our implementation: If you have a look at the scale of our data, it has changed quite a bit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11fe4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot filtered data and the original measurement data\n",
    "plt.figure()\n",
    "plt.plot(time_stamps, measurement_data['data_noise'])\n",
    "plt.plot(time_stamps, filter_data)\n",
    "plt.xlabel('Time'), plt.ylabel('Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067a8e9e",
   "metadata": {},
   "source": [
    "We can somewhat improve this by normalizing our scores to sum up to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915002a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normaliize scores by dividing by the sum of all scores\n",
    "scores_normalized = scores / np.sum(scores, axis=0)\n",
    "\n",
    "# Calculate the filtered data again using the normalized weights\n",
    "filter_data = scores_normalized @ measurement_data['data_noise']\n",
    "\n",
    "# Plot the filtered data\n",
    "plt.plot(time_stamps, filter_data)\n",
    "plt.xlabel('Time'), plt.ylabel('Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5b10a4",
   "metadata": {},
   "source": [
    "Now the scales should match a lot better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce607be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot filtered data with normalized scores and the original measurement data\n",
    "plt.figure()\n",
    "plt.plot(time_stamps, measurement_data['data_noise'])\n",
    "plt.plot(time_stamps, filter_data)\n",
    "plt.xlabel('Time'), plt.ylabel('Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f3e4cb",
   "metadata": {},
   "source": [
    "Another way of thinking about this, is that we have \"improved\" our data, by introducing context from other points to it, i.e. each datapoint looks at its neighboring datapoints. We can also introduce the terminology used in the paper: For every **query** (timestamp) $t_i$, compare it to all available **keys** $t_j$ (all other timestamps) and compute **similarity scores**. Multiply these scores with the corresponding **values** $x_j$ (the measurements) to form the \"answer\" to the query $t_i$. Note, that each key corresponds to a value!\n",
    "\n",
    "Now let's see if we can find something similar for words. After all, thats what we want our model to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65194c6c",
   "metadata": {},
   "source": [
    "### Applying Similarity-based Context to Word Embeddings\n",
    "\n",
    "The concept of attention mirrors the method we used earlier:\n",
    "\n",
    "When we transform a sentence into a sequence of embeddings $x_i$, translating each word independently may cause an issue: context matters!\n",
    "\n",
    "Consider these sentences:\n",
    "- The river bank is very long.\n",
    "- The bank is located at the long river.\n",
    "\n",
    "Both contain the word 'bank', but its meaning changes with context. Think about what this also means for the embeddings, which are supposed to \"capture the meaning\" of each word: The embedding for 'bank' should ideally differ in both sentences!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c6f96a",
   "metadata": {},
   "source": [
    "Our current implementation of the embedding layer does not take context (i.e. the other words in the sentence) into account, it simply maps each word to a vector from a static lookup table. Therefore, we need to find a way to **contextualize** the embeddings!\n",
    "\n",
    "The simplest way to create contextualized embeddings, similar to before, is to take a weighted average of the embeddings of all words we want to get context from - our values $x_j$:\n",
    "\n",
    "$\\tilde{x}_i = \\sum_{j=1}^N s_{ij} x_j$\n",
    "\n",
    "We define the scores as before, but now, we calculate similarity between the embeddings themselves! And just like in the noisy measurements, we compute the similarity between the query $x_i$ and the keys $x_j$: \n",
    "\n",
    "$s_{ij} = \\text{sim}(x_i, x_j)$\n",
    "\n",
    "Does this make sense? The learned embeddings ideally represent similar words that co-occur or are semantically linked (like 'apple' and 'fruit'). Conversely, words with low co-occurrence, like 'computer' and 'elephant', should be dissimilar. So in way it does make sense, that \"similar\" words should contribute more to each other's context (but we will revisit this later).\n",
    "\n",
    "Now, about the similarity function: instead of the squared exponential kernel, we use the dot product! But does the dot product truly measure similarity between vectors?\n",
    "\n",
    "$\\langle a,b \\rangle = \\cos(\\angle_{ab})|a||b|$\n",
    "\n",
    "We can see from here, that the closer two embeddings are, the higher the dot product will become! (Of course the norm of the vectors also affects the score)\n",
    "\n",
    "Let's define the similarity function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2546d9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define similarity function as the inner product\n",
    "def similarity(x1, x2):\n",
    "    return np.inner(x1, x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9099330",
   "metadata": {},
   "source": [
    "We have prepared a pretrained embedding model for you, so you can play around with it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d5af09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the word2vec model\n",
    "word2vec = util.load_word2vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2d1f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the word vectors for apple and fruit\n",
    "word_1 = word2vec['apple']\n",
    "word_2 = word2vec['fruit']\n",
    "\n",
    "# Compute the similarity between apple and fruit\n",
    "similarity(word_1, word_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c74c09",
   "metadata": {},
   "source": [
    "Ok, and now for a reference, let's look at two words that should not be related to much!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1be26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_1 = word2vec['computer']\n",
    "word_2 = word2vec['elephant']\n",
    "\n",
    "similarity(word_1, word_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f42476f",
   "metadata": {},
   "source": [
    "Ok, so we now what we have to do: Compute the similiarity scores to each key, and form the answer to our query. Here is a visualualization showing this flow: \n",
    "<!-- <img src=\"images/Transformer-AttentionMechanism.drawio.png\" width=2000> -->\n",
    "<img src=\"https://i2dl.vc.in.tum.de/static/images/Transformer-AttentionMechanism.drawio.png\" width=2000>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f18f992",
   "metadata": {},
   "source": [
    "Now let's do this in code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaea146f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an example sentence\n",
    "sentence = \"The river bank is very long\"\n",
    "\n",
    "# Get the word vectors for the words in the sentence\n",
    "embeddings, words = util.embedd_sentence(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22878f25",
   "metadata": {},
   "source": [
    "For now, we will set the queries, keys and values to be the same! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77817b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = embeddings\n",
    "keys = embeddings\n",
    "values = embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f68d83",
   "metadata": {},
   "source": [
    "Instead of calculating the score for each query separately, we can use matrix multiplication to calculate all the dot products between the query and keys in one go:\n",
    "\n",
    "$s_{ij} = q_i^T k_j$ or in matrix notation $S = QK^T$  \n",
    "\n",
    "This obviously only works if the matrices are oriented in the correct way! But we will be using the same format as we have been so far - each row is a sample (or in this case an embedded word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1284d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = query @ keys.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb43c82",
   "metadata": {},
   "source": [
    "Similar to the example with the timeseries, we want to make sure that the scores that correspond to a query all add to up one! We will be using the softmax in this case to ensure positive scores as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3507801",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = util.softmax(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda9a3ba",
   "metadata": {},
   "source": [
    "The final step is contextualizing the embeddings by summing over all values, weighted by the scores! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cba081",
   "metadata": {},
   "outputs": [],
   "source": [
    "contextualized_embeddings = scores @ values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e3b654",
   "metadata": {},
   "source": [
    "And voila, thats basically all there is to the attention mechanism! If we look at the formula in the paper you will find\n",
    "\n",
    "$Attention(Q,K,V) = softmax\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V$\n",
    "\n",
    "The value $d_k$ corresponds to the dimension of the keys and scales the dot product results. It is added to keep the dot product from growing, when the embedding dimensions is increased. (If you increase the dimension, you also increase the number of elements you have to add together!) However, larger values of the dot products push the softmax function into regions with smaller gradients, which can slow down training significantly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9461172d61753b1",
   "metadata": {},
   "source": [
    "<!-- <img src=\"images/Transformer-Attention Head.drawio.png\" width=\"2000\"> -->\n",
    "<img src=\"https://i2dl.vc.in.tum.de/static/images/Transformer-Attention Head.drawio.png\" width=2000>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8654ae2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 2: Implement</h3>\n",
    "    <p>Implement the <code>forward</code> pass in the class <code>ScaledDotAttention</code>. You can find it in <code>exercise_code/networks/attention.py</code>!\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f602f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you get an Error about SCORE_SAVER - please just restart your kernel!\n",
    "_ = test_task_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4faad5d9",
   "metadata": {},
   "source": [
    "## Multi-Head Attention\n",
    "\n",
    "So far, we are able to give context to an input sequence - awesome! However, what each token needs to \"pay attention to\" in its context is a bit more complicated! We can imagine, for example, that a verb might need to pay more attention to the subject and object of the sentence, or in the case of the word \"bank\", it might need to pay more attention to the word \"river\" or \"money\" to figure out its own intended meaning. And since language is so complex, you can imagine that there are so many different aspects that each token needs to pay attention to! This is where the idea of multi-head attention comes in:\n",
    "\n",
    "Instead of just using the embeddings directly, we can use linear transformations to down-project into more *fine-grained* queries $QW^Q$, keys $KW^K$ and values $VW^V$ from the embeddings. This way, the model can learn which aspects of the embeddings are important! We call this one \"unit\" of attention mechanism an **attention head**:\n",
    "\n",
    "$head(Q, K, V) = Attention(QW^Q, KW^K, VW^V)$\n",
    "\n",
    "And as the headline suggests, we can create multiple heads with their own weights and concatenate them together. This is very similar to a convolution layer with multiple filters in images! Each filter can learn different patterns in the image, and each head can learn different aspects to focus more attention to.\n",
    "\n",
    "Once all these heads are evaluated and concatenated, we have to somehow combine them together to retain the original shape of the input! This is done with a linear layer $W^O$:\n",
    "\n",
    "$MultiHead(Q, K, V) = Concat(head_1, ..., head_{n_{heads}})W^O$ where $head_i(Q, K, V) = Attention(QW_i^Q, KW_i^K, VW_i^V)$\n",
    "\n",
    "Finally, let's check the dimensions of the weights, where $d_{model}$ is the dimension of the embeddings:\n",
    "\n",
    "$shape(W^Q_i) = (d_{model},\\, d_q) = (d_{model},\\, d_k) $     Since we are computing dot products between the query vectors and the keys, the dimensions have to match!  \n",
    "$shape(W^K_i) = (d_{model},\\, d_k)$  \n",
    "$shape(W^V_i) = (d_{model},\\, d_v)$  \n",
    "$shape(W^O) = (d_v*n_{heads},\\, d_{model})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0e8c6d",
   "metadata": {},
   "source": [
    "<!-- <img src=\"images/Transformer-Multi Head Attention.drawio.png\" width=800> -->\n",
    "<img src=\"https://i2dl.vc.in.tum.de/static/images/Transformer-Multi Head Attention.drawio.png\" width=800>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f6f99378d59d12",
   "metadata": {},
   "source": [
    "In our implementation, we are going to vectorize this expressions by creating three large weight matrices instead of a set for each head:\n",
    "\n",
    "$shape(W^Q) = (d_{model},\\, n_{heads} \\cdot d_k)$       \n",
    "$shape(W^K) = (d_{model},\\, n_{heads} \\cdot d_k)$  \n",
    "$shape(W^V) = (d_{model},\\, n_{heads} \\cdot d_v)$  \n",
    "\n",
    "That way we don't have to loop through the heads in python! Now we have to reshape the outputs and get them into the correct shape! Let's have a look at one example:\n",
    "\n",
    "$shape(Q \\times W^Q) = (seq_Q, d_{model}) \\times (d_{model},\\, n_{heads} \\cdot d_k) = (seq_Q,\\, n_{heads} \\cdot d_k)$ where $seq_Q$ is the sequence length of the queries\n",
    "\n",
    "Now we have to split the last dimension into two parts - $n_{heads}$ and  $d_k$. We can do this using torch.reshape!:\n",
    "\n",
    "$(seq_Q,\\, n_{heads} \\cdot d_k)$ -> $(seq_Q,\\, n_{heads}, d_k)$\n",
    "\n",
    "Now if we look at the last two dimensions, we actually expect it to be $seq_Q$ and $d_k$, so that we can run it through the attention block! \n",
    "\n",
    "In other words, we have to swap (or better - transpose) the tensor at the correct dimension:\n",
    "\n",
    "$(seq_Q,\\, n_{heads}, d_k)$ -> $(n_{heads}, seq_Q,\\, d_k)$\n",
    "\n",
    "And now we can feed this into our attention mechanism as we usually do, think of it as a batch of batches. The pytorch functions automatically handle this for us! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cad2e95",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 3: Implement</h3>\n",
    "    <p>Implement the <code>__init__()</code> method and the <code>forward()</code> method of the <code>MultiHeadAttention</code> class in <code>exercise_code/network/multi_head_attention.py</code>. \n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9515509f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = test_task_3()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548d0ecf3b6214a5",
   "metadata": {},
   "source": [
    "# Cross Attention vs Self Attention\n",
    "\n",
    "You might have noticed, that we are always talking about inputs to an attention mechanism and the context we want to compare it to. If we use the terminology from the paper, our inputs form the query of our attention mechanism, and the context forms the key - value pairs! \n",
    "\n",
    "## Cross Attention\n",
    "Cross-Attention might be the easier idea to understand. We use two different sources for the input and context! In the transformer model, this type of attention will be used in the decoder, that way we can contextualize our output with the actual input to our model. \n",
    "\n",
    "Let's say you ask the model a question like \"Hello, how are you?\". This will be processed by the encoder and it will give us some output. Next, the decoder will start to produce its output token by token, and at each step, the output should obviously depend on our initial question! We can achieve this by using cross attention, where the inputs to the mechanism come from the decoder itself, and the context comes from the encoder. Is is trying to give context from our input to its output - something we definetly want to have!\n",
    "\n",
    "## Self Attention\n",
    "In Self-Attention, the input and the context source are the same! That means, every word in a sequence is attending to all words in the same sequence, or in other words - the sentence is attending to itself. This form is used in both the encoder and decoder, and is used to process their inputs to gain a \"deeper\" understanding of the inputs by giving them context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f872475",
   "metadata": {},
   "source": [
    "<!-- <img src=\"images/Transformer-Self Cross Attention.drawio.png\" width=2000> -->\n",
    "<img src=\"https://i2dl.vc.in.tum.de/static/images/Transformer-Self Cross Attention.drawio.png\" width=2000>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc78782",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "\n",
    "The model we have discussed so far does not care about the relative positions of the words in the sentences! The scores between two words is the same, no matter whether they are next to each other or far apart from each other! In other models like RNN we can feed in the words sequentially. With convolutions, the relative neighbourhood of a word is taken into account. In both models, the order is automatically taken into account.\n",
    "\n",
    "Does order matter?\n",
    "\n",
    "Consider these sentences again:\n",
    "- The river bank is very long.\n",
    "- The bank is located at the long river.\n",
    "\n",
    "Depending on the location of the word bank in the sentence, the meaning of the word completely changes. Our model however wouldn't be able to tell them apart. To solve this issue, we can add a positional encoding, think of it as bias, that we add to our embeddings. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa1117d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>WARNING</h3>\n",
    "    <p>The next section sort of tries to give you an intuition where the formulas for the positional encoding come from! You do not have to go through every single math equation or code line in this section, rather have a look at the graphs and try to get a feeling for what is going on! \n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a008ca161f44de",
   "metadata": {},
   "source": [
    "### Intuition behind Positional Encoding\n",
    "\n",
    "Suppose we use a 4d embedding and we want to encode the position of a token. A possible choice could be to simply use the binary representation of the position as an encoding. So in other words, to the token at position 5, we would add the vector [0 1 0 1]. Here are all the binary representations up to 15:\n",
    "\n",
    "\n",
    "| Decimal                      | Binary                                      | Decimal                                  | Binary |\n",
    "|-------------------------------------|----------------------------------------------------|--------------------------------------------------|--|\n",
    "| 00 | <span style=\"color: green;\">0</span> <span style=\"color: orange;\">0</span> <span style=\"color: red;\">0</span> <span style=\"color: cyan;\">0</span> | 08 | <span style=\"color: green;\">1</span> <span style=\"color: orange;\">0</span> <span style=\"color: red;\">0</span> <span style=\"color: cyan;\">0</span> </br> |\n",
    "| 01 | <span style=\"color: green;\">0</span> <span style=\"color: orange;\">0</span> <span style=\"color: red;\">0</span> <span style=\"color: cyan;\">1</span> | 09 | <span style=\"color: green;\">1</span> <span style=\"color: orange;\">0</span> <span style=\"color: red;\">0</span> <span style=\"color: cyan;\">1</span> </br> |\n",
    "| 02 | <span style=\"color: green;\">0</span> <span style=\"color: orange;\">0</span> <span style=\"color: red;\">1</span> <span style=\"color: cyan;\">0</span> | 10 | <span style=\"color: green;\">1</span> <span style=\"color: orange;\">0</span> <span style=\"color: red;\">1</span> <span style=\"color: cyan;\">0</span> </br> |\n",
    "| 03 | <span style=\"color: green;\">0</span> <span style=\"color: orange;\">0</span> <span style=\"color: red;\">1</span> <span style=\"color: cyan;\">1</span> | 11 | <span style=\"color: green;\">1</span> <span style=\"color: orange;\">0</span> <span style=\"color: red;\">1</span> <span style=\"color: cyan;\">1</span> </br> |\n",
    "| 04 | <span style=\"color: green;\">0</span> <span style=\"color: orange;\">1</span> <span style=\"color: red;\">0</span> <span style=\"color: cyan;\">0</span> | 12 | <span style=\"color: green;\">1</span> <span style=\"color: orange;\">1</span> <span style=\"color: red;\">0</span> <span style=\"color: cyan;\">0</span> </br> |\n",
    "| 05 | <span style=\"color: green;\">0</span> <span style=\"color: orange;\">1</span> <span style=\"color: red;\">0</span> <span style=\"color: cyan;\">1</span> | 13 | <span style=\"color: green;\">1</span> <span style=\"color: orange;\">1</span> <span style=\"color: red;\">0</span> <span style=\"color: cyan;\">1</span> </br> |\n",
    "| 06 | <span style=\"color: green;\">0</span> <span style=\"color: orange;\">1</span> <span style=\"color: red;\">1</span> <span style=\"color: cyan;\">0</span> | 14 | <span style=\"color: green;\">1</span> <span style=\"color: orange;\">1</span> <span style=\"color: red;\">1</span> <span style=\"color: cyan;\">0</span> </br> |\n",
    "| 07 | <span style=\"color: green;\">0</span> <span style=\"color: orange;\">1</span> <span style=\"color: red;\">1</span> <span style=\"color: cyan;\">1</span> | 15 | <span style=\"color: green;\">1</span> <span style=\"color: orange;\">1</span> <span style=\"color: red;\">1</span> <span style=\"color: cyan;\">1</span> </br> |\n",
    "\n",
    "Let's plot this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef61ef30996801bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are flipping the bits around, so that the least significant bit is plotted first! This will make the formulas a little easier later!\n",
    "pos_encoding = np.flip(np.array([[0, 0, 0, 0],\n",
    "                                 [0, 0, 0, 1],\n",
    "                                 [0, 0, 1, 0],\n",
    "                                 [0, 0, 1, 1],\n",
    "                                 [0, 1, 0, 0],\n",
    "                                 [0, 1, 0, 1],\n",
    "                                 [0, 1, 1, 0],\n",
    "                                 [0, 1, 1, 1],\n",
    "                                 [1, 0, 0, 0],\n",
    "                                 [1, 0, 0, 1],\n",
    "                                 [1, 0, 1, 0],\n",
    "                                 [1, 0, 1, 1],\n",
    "                                 [1, 1, 0, 0],\n",
    "                                 [1, 1, 0, 1],\n",
    "                                 [1, 1, 1, 0],\n",
    "                                 [1, 1, 1, 1]]), axis=-1)\n",
    "\n",
    "util.plot_positional_encoding(pos_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4060a487",
   "metadata": {},
   "source": [
    "As you can see, every bit is jumping between 0 and 1 with a different frequency! </br>\n",
    "The <span style=\"color: cyan;\">0th bit</span> is jumping back and forth after 1 iteration. </br>\n",
    "The <span style=\"color: red;\">1st bit</span> is jumping back and forth after 2 iteration. </br>\n",
    "The <span style=\"color: orange;\">2nd bit</span> is jumping back and forth after 4 iteration. </br>\n",
    "The <span style=\"color: green;\">3rd bit</span> is jumping back and forth after 8 iteration. </br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f390b7f5940f9d",
   "metadata": {},
   "source": [
    "We can get the same pattern using a sine wave with different frequencies! In a first step, let's just have a look at the sign (+/-) of the sine function!\n",
    "\n",
    "$ PE(pos) = \\begin{bmatrix}\n",
    "           -sin(\\frac{\\pi}{1} \\cdot (pos + 0.5)) > 0 \\\\\n",
    "           -sin(\\frac{\\pi}{2} \\cdot (pos + 0.5)) > 0 \\\\\n",
    "           -sin(\\frac{\\pi}{4} \\cdot (pos + 0.5)) > 0 \\\\\n",
    "           -sin(\\frac{\\pi}{8} \\cdot (pos + 0.5)) > 0\n",
    "         \\end{bmatrix} $\n",
    "         \n",
    " You might be surprised to see the negative sine here! This only because we wanted it to start negative (=0) for the first half and then become positive (=1) in the second half to match our bit pattern! Also, the pos + 0.5 might be a bit weird at first, but we only did this to compute the value of the sine wave in the middle of each cell! But it is not important, just have a look at the plot of this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf53700caab9ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding_discrete(pos):\n",
    "    return np.array([-np.sin((np.pi / 1) * (pos + 0.5)) > 0,        \n",
    "                     -np.sin((np.pi / 2) * (pos + 0.5)) > 0,\n",
    "                     -np.sin((np.pi / 4) * (pos + 0.5)) > 0,\n",
    "                     -np.sin((np.pi / 8) * (pos + 0.5)) > 0]).T\n",
    "\n",
    "positions = np.arange(0, 16)\n",
    "\n",
    "util.plot_positional_encoding(positional_encoding_discrete, \n",
    "                              positions=positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ae4b4d03bb1fef",
   "metadata": {},
   "source": [
    "Tadaaaa - Same pattern!\n",
    "\n",
    "Now instead of using only the integers 0 and 1, let's use all values between 0 and 1! We will keep the rest the same for now! (That means we get rid of the sign() function, or the > 0 in code!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca10699c9b910e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding_continuous(pos):\n",
    "    return np.array([-np.sin((np.pi / 1) * (pos + 0.5)),    \n",
    "                     -np.sin((np.pi / 2) * (pos + 0.5)),\n",
    "                     -np.sin((np.pi / 4) * (pos + 0.5)),\n",
    "                     -np.sin((np.pi / 8) * (pos + 0.5))]).T\n",
    "\n",
    "positions = np.arange(0, 16)\n",
    "\n",
    "util.plot_positional_encoding(positional_encoding_continuous, \n",
    "                              positional_encoding_discrete, \n",
    "                              positions=positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cac077879c9ab8d",
   "metadata": {},
   "source": [
    "You should still sort of be able to see the original pattern, its just a bit blurred now!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93438f1a7999a0b2",
   "metadata": {},
   "source": [
    "From here, we are going to do a couple alterations to the formula: First of all, get rid of the pos + 0.5! That really was only there, to show the similarity to the bit pattern!\n",
    "\n",
    "$ PE(pos) = \\begin{bmatrix}\n",
    "           sin(\\frac{\\pi}{1} \\cdot pos) \\\\\n",
    "           sin(\\frac{\\pi}{2} \\cdot pos) \\\\\n",
    "           sin(\\frac{\\pi}{4} \\cdot pos) \\\\\n",
    "           sin(\\frac{\\pi}{8} \\cdot pos)\n",
    "         \\end{bmatrix} $\n",
    "\n",
    "In a next step, we will also get rid of scaling frequency by $\\pi$! This leads us to:\n",
    " \n",
    "$ PE(pos) = \\begin{bmatrix}\n",
    "           sin(\\frac{1}{1} \\cdot pos) \\\\\n",
    "           sin(\\frac{1}{2} \\cdot pos) \\\\\n",
    "           sin(\\frac{1}{4} \\cdot pos) \\\\\n",
    "           sin(\\frac{1}{8} \\cdot pos)\n",
    "         \\end{bmatrix} $\n",
    "         \n",
    "Let's have a closer look at these angle frequencies:\n",
    "\n",
    "$\\omega_0 = \\frac{1}{1} =  \\frac{1}{2}^0 $ \\\n",
    "$\\omega_1 = \\frac{1}{2} =  \\frac{1}{2}^1 $ \\\n",
    "$\\omega_2 = \\frac{1}{4} =  \\frac{1}{2}^2 $ \\\n",
    "$\\omega_3 = \\frac{1}{8} =  \\frac{1}{2}^3 $ \n",
    "\n",
    "The frequencies form a geometric series with base $\\frac{1}{2}$! Writing this as a formula with base b we get:\n",
    "\n",
    "$\\omega_i = b^i$\n",
    "\n",
    "Now one last change we are going to do is to add a factor d in to the exponent as follows:\n",
    "\n",
    "$\\omega_i = b^{i/d}$\n",
    "\n",
    "Remeber - i here denotes the i-th dimension of our positional encoding vector! Each Dimension \"vibrates\" at a it's own frequency!\n",
    "\n",
    "This results in the follwing formula:\n",
    "\n",
    "$ PE(pos, i) = sin(b^{i/d} \\cdot pos)$\n",
    "\n",
    "Let's have a look at the final outcome for different d's! (This will sort of look like we are zooming!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d723f81c28c5ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos, base, d):\n",
    "    return np.array([np.sin(base ** 0/d * pos),    \n",
    "                     np.sin(base ** 1/d * pos),\n",
    "                     np.sin(base ** 2/d * pos),\n",
    "                     np.sin(base ** 3/d * pos)]).T\n",
    "\n",
    "positions = np.arange(0, 16)\n",
    "\n",
    "d_factors = [1, 2, 5, 10]  # Different zoom factors for the plots\n",
    "\n",
    "util.plot_positional_encoding(positional_encoding, \n",
    "                              positions=positions, \n",
    "                              d_factors=d_factors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2899be73355355",
   "metadata": {},
   "source": [
    "Puh, that was a lot! Now with all this prep, the actual formulas shouldn't come as that big of a surprise anymore!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8156af05ac23e51e",
   "metadata": {},
   "source": [
    "### Implementing Positional Encodings\n",
    "\n",
    "The positional encodings used in the paper ar actually very similar\n",
    "\n",
    "$PE(pos, 2i) = \\sin(pos / 10000 ^{2i/d})$ \\\n",
    "$PE(pos, 2i+1) = \\cos(pos / 10000 ^{2i/d})$\n",
    "\n",
    "At a first glance, there is a lot going on here! Rewriting the formula actually helps a lot!\n",
    "\n",
    "$ PE(pos) = \\begin{bmatrix}\n",
    "           sin(w_0 \\cdot pos) \\\\\n",
    "           cos(w_0 \\cdot pos) \\\\\n",
    "           sin(w_1 \\cdot pos) \\\\\n",
    "           cos(w_1 \\cdot pos) \\\\\n",
    "           \\vdots \\\\\n",
    "           sin(w_{d/2} \\cdot pos) \\\\\n",
    "           cos(w_{d/2} \\cdot pos)\n",
    "         \\end{bmatrix} $\n",
    "\n",
    "With:\n",
    "$ \\omega_i = \\frac{1}{10000}^{2i/d} $ \n",
    "\n",
    "Where\n",
    "\n",
    "$pos$ refers to the position of the token in the sequence </br>\n",
    "$d$ refers to the dimension of the embedding (=d_model)\n",
    "\n",
    "Instead of only using sine functions, they also used cosine functions at every other index (note: because of this, you should always use an even embedding dimension!) </br>\n",
    "In the original formula, the \"every other index\" is noted by $2i$ and $2i+1$, which basically means all even indices get sine functions and all uneven cosine! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143baed9174cb10",
   "metadata": {},
   "source": [
    "Some of you might be curious, why they added the cosine function!\n",
    "\n",
    "To quote the paper:\n",
    "\n",
    "> We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, PE(pos+k) can be represented as a linear function of PE(pos).\n",
    "\n",
    "Without the cosine function, this cannot be done anymore!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da2bb3ae45d691e",
   "metadata": {},
   "source": [
    "Anyway, let's have a look at this positional encoding! Since this encoding doesn't depend on data and is constant over training, we can compute it one time and store it as a large tensor as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e142d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(length, depth):\n",
    "\n",
    "    i = np.arange(0, depth, 2) / depth\n",
    "    pos = np.arange(0, length)[:, None]\n",
    "\n",
    "    angle_freq = np.exp(i * (-np.log(10000)))   # For numerical reasons\n",
    "    \n",
    "    pos_encoding = np.zeros((length, depth))\n",
    "    \n",
    "    pos_encoding[:, 0::2] = np.sin(pos * angle_freq)\n",
    "    pos_encoding[:, 1::2] = np.cos(pos * angle_freq)\n",
    "\n",
    "    return pos_encoding\n",
    "\n",
    "util.plot_positional_encoding(positional_encoding,\n",
    "                              length=2048, \n",
    "                              depth=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d737d1d",
   "metadata": {},
   "source": [
    "The version I imagine you are probably more familiar with is the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742a6c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(length, depth):\n",
    "\n",
    "    i = np.arange(0, depth, 2) / depth\n",
    "    pos = np.arange(0, length)[:, None]\n",
    "\n",
    "    angle_freq = np.exp(i * (-np.log(10000)))\n",
    "\n",
    "    pos_encoding = np.concatenate([np.sin(pos * angle_freq), np.cos(pos * angle_freq)], axis=-1)\n",
    "\n",
    "    return pos_encoding\n",
    "\n",
    "util.plot_positional_encoding(positional_encoding,\n",
    "                              length=2048, \n",
    "                              depth=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c954072c",
   "metadata": {},
   "source": [
    "The only difference is that instead of using sine for the even and cosine for the uneven we just concatenate the two vectors. Both versions are valid, since the main thing we want to achieve with this encoding is that vectors close by have a higher score and vectors further apart have a lower score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69286b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_encoding = positional_encoding(length=2048, depth=512)\n",
    "pos_encoding = pos_encoding / np.linalg.norm(pos_encoding, axis=-1, keepdims=True)\n",
    "\n",
    "p = pos_encoding[1000]\n",
    "scores = pos_encoding @ p\n",
    "\n",
    "util.plot_scores(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d575fbf",
   "metadata": {},
   "source": [
    "And as promised, here is our full embedding layer:\n",
    "\n",
    "<!-- <img src=\"images/Transformer-Embedding + Positional Encoding.drawio.png\" width=\"1000\"> -->\n",
    "<img src=\"https://i2dl.vc.in.tum.de/static/images/Transformer-Embedding + Positional Encoding.drawio.png\" width=1000>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c71a378",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 4: Implement</h3>\n",
    "    <p>Add the positional encoding to the <code>Embedding</code> class in <code>exercise_code/network/embedding.py</code>.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a171f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = test_task_4()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435f16f75c0b5bae",
   "metadata": {},
   "source": [
    "## Encoder Block\n",
    "\n",
    "The encoder side of the transformer processes the input to the model. If your model is trained on translating sentences, the input will be the sentence in the source language. \n",
    "\n",
    "Each Block consists of \n",
    "\n",
    "1. **multi-head self-attention layer**\n",
    "2. **residual connection**\n",
    "3. **layer normalization**\n",
    "4. **feed forward network**\n",
    "5. **residual connection**\n",
    "6. **layer normalization**\n",
    "\n",
    "We use layer normalization instead of batch normalization to get similar advantages of improving training stability while decoupling it from the batch size. (Remember, in batch normalization we normalize over an entire batch, while in layer normalization we normalize across over all inputs of a single sample!)\n",
    "\n",
    "The feed forward network is applied to each token embedding separately with follwing architecture: \n",
    "\n",
    "$FFN(x) = RELU(xW_1 + b_1)W_2 + b_2$,  where\n",
    "\n",
    "$shape(W_1) = (d_{model},\\, d_{ff})$ </br>\n",
    "$shape(W_2) = (d_{ff},\\, d_{model})$\n",
    "\n",
    "In our model we will use an inner dimension of $d_{ff} = 2048$.\n",
    "After this, another residual connection followed by a layer normalization is added.\n",
    "\n",
    "One intuitive way to think about this $MultiHead \\rightarrow FFN$ structure of the encoder block is that the attention mechanism is used to \"look around\" in the input sequence and the feed forward network is used to process the information and transform it into a more useful representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1971a5",
   "metadata": {},
   "source": [
    "<!-- <img src=\"images/Transformer-Encoder Block.drawio.png\" width=\"1300\"> -->\n",
    "<img src=\"https://i2dl.vc.in.tum.de/static/images/Transformer-Encoder Block.drawio.png\" width=1300>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb92f5506c45c2a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 5: Implement</h3>\n",
    "    <p>Implement the <code>__init__()</code> method and the <code>forward()</code> method of the <code>FeedForwardNeuralNetwork</code> class in <code>exercise_code/network/nn.py</code>.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ab94c21859e7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = test_task_5()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1ce69f250dbc3e",
   "metadata": {},
   "source": [
    "<div class =\"alert alert-info\">\n",
    "    <h3>Task 6: Implement </h3>\n",
    "    <p>Implement the <code>__init__()</code> method and the <code>forward()</code> method of the <code>EncoderBlock</code> class in <code>exercise_code/network/encoder_block.py</code>.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6263d0344428b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = test_task_6()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09703b84",
   "metadata": {},
   "source": [
    "## Encoder Stack\n",
    "\n",
    "The only part on the encoder side that is left is to stack multiple blocks together!\n",
    "\n",
    "<!-- <img src=\"images/Transformer-Encoder.drawio.png\" width=\"1000\"> -->\n",
    "<img src=\"https://i2dl.vc.in.tum.de/static/images/Transformer-Encoder.drawio.png\" width=1000>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9ca607",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 7: Check Code</h3>\n",
    "    <p>Check the <code>__init__()</code> method and the <code>forward()</code> method of the <code>Encoder</code> class in <code>exercise_code/network/encoder.py</code>.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74017db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = test_task_7()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b46041",
   "metadata": {},
   "source": [
    "# Decoder\n",
    "The job of the decoder is basically to produce an output given an input and the previous outputs (if available). Those previous outputs are also the input to the decoder! We have actually already seen most of the relevant parts of the decoder in the encoder, there is only one major addition: Causal attention!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b573662",
   "metadata": {},
   "source": [
    "## Causal Attention\n",
    "\n",
    "Previous language models based on RNN have one large draw back: during training, the model has to iteratively go through the entire sequence to predict the next word. Transformer models on the other hand can do this in parallel!\n",
    "\n",
    "So instead of something like this:\n",
    "\n",
    "|                    | Iteration 1                        | Iteration 2                        | Iteration 3                        |  \n",
    "|--------------------|------------------------------------|------------------------------------|------------------------------------|\n",
    "| **Encoder Input**  | [\"Hello\" ,\"how\", \"are\", \"you\", \"?\"]| [\"Hello\" ,\"how\", \"are\", \"you\", \"?\"]| [\"Hello\" ,\"how\", \"are\", \"you\", \"?\"]|\n",
    "| **Decoder Input**  | [\"\\<start>\"]                       | [\"\\<start>\", \"Nicht\"]              | [\"\\<start>\", \"Nicht\", \"sehr\"]      | \n",
    "| **Decoder Output** | [\"Nicht\"]                          | [\"Nicht\", \"sehr\"]                  | [\"Nicht\", \"sehr\", \"effektiv\"]      | \n",
    "| **Compare to**     | [\"Hallo\"]                          | [\"Hallo\", \"wie\"]                   | [\"Hallo\", \"wie\", \"geht's\"]         | \n",
    "\n",
    "\n",
    "and so on we want to do this in one pass, where we give the model the correct sentence as the decoder input. It is shifted right, since the model should predict the first token, given the \\<start> token.\n",
    "\n",
    "|                    | Iteration 1                                        |\n",
    "|--------------------|----------------------------------------------------|\n",
    "| **Encoder Input**  | [\"Hello\" ,\"how\", \"are\", \"you\", \"?\"]                |\n",
    "| **Decoder Input**  | [\"\\<start>\", \"Hallo\", \"wie\", \"geht's\", \"dir\", \"?\"] | \n",
    "| **Decoder Output** | [\"Hallo\", \"wie\", \"geht's\", \"dir\", \"?\", \"\\<end>\"]   | \n",
    "| **Compare to**     | [\"Hallo\", \"wie\", \"geht's\", \"dir\", \"?\", \"\\<end>\"]   | \n",
    "\n",
    "\n",
    "Problem is, our model could theoretically learn to cheat, by just returning the same sequence it got as an input. In other words, we want to ensure, that when the model is predicting the token [\"geht's\"], it only depends on the previous token [\"\\<start>\", \"Hallo\", \"wie\"]. This can be done with masks!\n",
    "\n",
    "If we look back at our definition of attention we had:\n",
    "\n",
    "$\\tilde{x}_i = \\sum_{j=1}^N s_{ij} x_j$\n",
    "\n",
    "If we dont want future tokens to have an affect on the current token, we have to ensure, that all scores where $j > i$ are zero! (Normally it would be $\\geq$, but since the decoder input is shifted over by one token - the \\<start> token - its $>$)\n",
    "\n",
    "Example:\n",
    "\n",
    "$\\tilde{x}_0 = s_{00} x_0 + s_{01} x_1 +s_{02} x_2$ \\\n",
    "$\\tilde{x}_1 = s_{10} x_0 + s_{11} x_1 +s_{12} x_2$ \\\n",
    "$\\tilde{x}_2 = s_{20} x_0 + s_{21} x_1 +s_{22} x_2$  \n",
    "\n",
    "Now, we want  \n",
    "$\\tilde{x}_0$ to only depend on $x_0$  \n",
    "$\\tilde{x}_1$ to only depend on $x_0$ and $x_1$  \n",
    "$\\tilde{x}_2$ to only depend on $x_0$, $x_1$ and $x_2$  \n",
    "\n",
    "That leads to:  \n",
    "$\\tilde{x}_0 = s_{00} x_0 + 0 \\cdot x_1 + 0 \\cdot x_2$  \n",
    "$\\tilde{x}_1 = s_{10} x_0 + s_{11} x_1 + 0 \\cdot x_2$  \n",
    "$\\tilde{x}_2 = s_{20} x_0 + s_{21} x_1 +s_{22} x_2$  \n",
    "\n",
    "The scores form a lower triangle matrix. Let's test this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8709b828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define scores as lower triangular matrix of ones\n",
    "scores = np.tril(np.ones((4, 4)), k=0)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57eb4411",
   "metadata": {},
   "source": [
    "Now that we know what we have to do, we have figure when to set these scores to zero! We could try right after we compute the dot products using a mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878371f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define mask as lower triangular matrix of ones (same as above)\n",
    "mask = np.tril(np.ones((4, 4)), k=0)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f03abe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dummy embeddings fo this example\n",
    "queries = util.get_dummy_embeddings()\n",
    "keys = util.get_dummy_embeddings()\n",
    "\n",
    "# Compute scores\n",
    "scores = queries @ keys.T\n",
    "\n",
    "# Multiply scores with mask to set all scores above the diagonal to zero\n",
    "scores = scores * mask\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680aa112",
   "metadata": {},
   "source": [
    "Looks good! But what happens when we run the softmax over it? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa748c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply softmax to scores\n",
    "scores = util.softmax(scores)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8006e35f",
   "metadata": {},
   "source": [
    "Suddenly are weights arent zero anymore! This shouldn't be that big of a surprise, since $e^0 = 1$.\n",
    "Alright, let's try setting the scores to zero after the softmax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c269f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute scores\n",
    "scores = queries @ keys.T\n",
    "\n",
    "# Apply softmax to scores\n",
    "scores = util.softmax(scores)\n",
    "\n",
    "# Multiply scores with mask to set all scores above the diagonal to zero\n",
    "scores = scores * mask\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab35398",
   "metadata": {},
   "source": [
    "Alright, looks better! Only problem left is that the values don't add up to 1 anymore for every column! We somehow have to change the values before the softmax is applied!\n",
    "That's exactly what infinity mask do! Remember $e^{-inf} = 0$! (Technically it's the limit, but I think you know what we mean!) So these values wouldn't affect the sum, and their value will automatically be zero! So all we have to do is add -inf to the values we want to be zero later on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21235cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all zeros with -inf (We want to keep the scores with a one!)\n",
    "inf_mask = np.where(mask, 0, -np.inf)\n",
    "print(inf_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fc95f8",
   "metadata": {},
   "source": [
    "Let's add this to our scores before the softmax is applied:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2214367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute scores\n",
    "scores = (queries @ keys.T) \n",
    "\n",
    "# Add the -inf mask to the scores\n",
    "scores += inf_mask\n",
    "\n",
    "# Apply softmax to scores\n",
    "scores = util.softmax(scores)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d55fa6",
   "metadata": {},
   "source": [
    "Perfect! Everything seems to work as planned!\n",
    "\n",
    "Note: This perticular kind of mask is known as a casual mask. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a42e82",
   "metadata": {},
   "source": [
    "<!-- <img src=\"images/Transformer-Masked Attention.drawio.png\" width=\"1000\"> -->\n",
    "<img src=\"https://i2dl.vc.in.tum.de/static/images/Transformer-Masked Attention.drawio.png\" width=1000>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf41b774",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 8: Implement</h3>\n",
    "    <p>Apply masking as explained above in the <code>forward()</code> method of the <code>ScaledDotAttention</code> class and update the <code>forward()</code> method of the <code>MultiHeadAttention</code> class to pass the mask to the attention heads!\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47805b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = test_task_8()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc01d5e",
   "metadata": {},
   "source": [
    "## Decoder Block\n",
    "Just like the encoder, the decoder consists of several blocks. \n",
    "\n",
    "Each Block consists of \n",
    "\n",
    "1. **causal multi-head self-attention layer**\n",
    "2. **residual connection**\n",
    "3. **layer normalization**\n",
    "4. **multi-head cross-attention layer**\n",
    "5. **residual connection**\n",
    "6. **layer normalization**\n",
    "7. **feed forward network**\n",
    "8. **residual connection**\n",
    "9. **layer normalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c603d49d",
   "metadata": {},
   "source": [
    "<!-- <img src=\"images/Transformer-DecoderBlock.drawio.png\" width=\"1500\"> -->\n",
    "<img src=\"https://i2dl.vc.in.tum.de/static/images/Transformer-Decoder Block.drawio.png\" width=1500>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386125a4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 9: Implement</h3>\n",
    "    <p>Implement the <code>__init__()</code> method and the <code>forward()</code> method of the <code>DecoderBlock</code> class in <code>exercise_code/network/decoder_block.py</code>.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235605a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = test_task_9()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8e4c81",
   "metadata": {},
   "source": [
    "## Decoder Stack\n",
    "\n",
    "Just like in the encoder, the decoder consist of several decoder blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef132331",
   "metadata": {},
   "source": [
    "<!-- <img src=\"images/Transformer-Decoder.drawio.png\" width=\"1000\"> -->\n",
    "<img src=\"https://i2dl.vc.in.tum.de/static/images/Transformer-Decoder.drawio.png\" width=1000>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc37d1b2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 10: Check Code</h3>\n",
    "    <p>Check the <code>__init__()</code> method and the <code>forward()</code> method of the <code>Decoder</code> class in <code>exercise_code/network/decoder.py</code>.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9370b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = test_task_10()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a911f69",
   "metadata": {},
   "source": [
    "## Final Output\n",
    "\n",
    "The last thing we have to do is project the individual embeddings into distributions over our vocabulary. This can be done with a simple linear layer! The outputs at this stage will not be distributions yet, since the values do not add up to one! We accomplish this in the loss layer, using a softmax function!\n",
    "\n",
    "To minimize the weights needed in this network, we will actually use a technique called weight tying! If you remember the Embedding Layer, this was basically a weight matrix with shape (vocab_size, d_model). For our final output layer, we want to project from the embedding space to the vocabulary space, so that gives us (d_model, vocab_size)! The shapes are just transposed! And the approach is to actually reuse these weights from our embeddings and transpose them for our final layer! Here is the corresponding paper: https://arxiv.org/abs/1608.05859v3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd491ec",
   "metadata": {},
   "source": [
    "<!-- <img src=\"images/Transformer-Transformer-Full.drawio.png\" width=\"2500\"> -->\n",
    "<img src=\"https://i2dl.vc.in.tum.de/static/images/Transformer-Transformer-Full.drawio.png\" width=2500>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5778c96",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 11: Implement</h3>\n",
    "    <p>Implement the <code>__init__()</code> and the <code>forward()</code> method of the <code>Transformer</code> class in <code>exercise_code/network/transformer.py</code>. We have already implemented weight tying for you!\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9e6a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = test_task_11()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569366d6",
   "metadata": {},
   "source": [
    "# Paddings\n",
    "\n",
    "A keen eye might have noticed a problem with our model - in the collate function we added paddings to ensure all sequences have the same length! Our model on the other hand shouldn't change its output just because we are adding \"empty\" tokens at the end of our sequence! The good news is, we have actually already implemented most of what we need to actually enable this! But first, let's have a look at what we are dealing with and load in a batch from our dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3116f44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.data import CustomIterableDataset\n",
    "from exercise_code.data import CustomCollator\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the path to the dataset\n",
    "file = os.path.join(dataset_path, 'dummyDatasets', 'ds_dummy')\n",
    "\n",
    "# Define the collator and dataset\n",
    "collator = CustomCollator()\n",
    "dataset = CustomIterableDataset(file)\n",
    "\n",
    "# Define the data loader\n",
    "loader = DataLoader(dataset, batch_size=3, collate_fn=collator)\n",
    "\n",
    "# Create an Embedding layer with 512 dimensions\n",
    "embedding = Embedding(vocab_size=len(collator.tokenizer), d_model=512, max_length=2048)\n",
    "\n",
    "# Get the first batch from the data loader\n",
    "batch = next(iter(loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079c4025",
   "metadata": {},
   "source": [
    "Note: If this didn't work, there is probably a problem in your Dataset! Go Back to Notebook 2 and make sure you pass the test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb47e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_masks = batch['encoder_mask']\n",
    "\n",
    "print(padding_masks.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2089927b",
   "metadata": {},
   "source": [
    "In this first item of the batch, the sequence has no padding at all - so nothing to do here!\n",
    "For all the others we have come up with something... Let's go back to the formula:\n",
    "\n",
    "$\\tilde{x}_i = \\sum_{j=1}^N s_{ij} x_j$\n",
    "\n",
    "All we really want is that if $x_j$ is a padding token, it doesn't contribute to the updated embedding! In other words - its score has to be zero! We can solve this exactly the same way as we did with the attention masking! Let's do this for a single item, in this case the second item in the batch!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7edee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the embeddings from the second item in the batch\n",
    "inputs = embedding(batch['encoder_inputs'][1])\n",
    "\n",
    "# We are using the same embeddings for the queries, keys and values - self-attention!\n",
    "queries = inputs\n",
    "keys = inputs\n",
    "values = inputs\n",
    "\n",
    "print(queries.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f737bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the padding mask of the second item\n",
    "padding_mask = padding_masks[1].squeeze(0)\n",
    "\n",
    "print(padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbcb3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask of length 12 by 12 and set the values to 0 where we have to mask\n",
    "mask = torch.ones((len(padding_mask), len(padding_mask)))\n",
    "\n",
    "for i, row in enumerate(mask):\n",
    "    for j, item in enumerate(row):\n",
    "        if not padding_mask[j]:\n",
    "            mask[i, j] = 0\n",
    "    \n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cbe5e1",
   "metadata": {},
   "source": [
    "What we did is set the mask to zero, for every column that refers to a padding token! What we get is this matrix, where the left side are ones, and the rest is zero. We can achieve the same result by just copying the vector along the first dimension - duh! (We will let pytorch handle the copying automatically using broadcasting, a keen eye might have noticed that we squeezed the dimension two cells up ;))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d325b6",
   "metadata": {},
   "source": [
    "From here we can treat it the same way we did with our causal attention block, by adding -inf to all values we want to mask out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb33053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax\n",
    "\n",
    "# Compute scores\n",
    "scores = (queries @ keys.T)/np.sqrt(512)\n",
    "\n",
    "# Instead of adding -inf, we set the scores to -inf where the mask is 0 -> Same thing ;)\n",
    "scores.masked_fill_(~mask.bool(), -torch.inf)\n",
    "\n",
    "# Apply softmax to scores\n",
    "scores = softmax(scores, dim=-1)\n",
    "\n",
    "# Print scores\n",
    "scores = scores.detach().numpy()\n",
    "util.plot_attention_scores(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ec363f",
   "metadata": {},
   "source": [
    "Perfect, the scores are concentrated to the left side!\n",
    "For the causal mask (the lower triangle mask) we have to combine it with the decoder mask. We can do this by simply multiplying the two together!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e75ba46e26ab345",
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.util.transformer_util import create_causal_mask\n",
    "\n",
    "# Get the masks from the first batch\n",
    "encoder_mask = batch['encoder_mask']\n",
    "decoder_mask = batch['decoder_mask']\n",
    "\n",
    "# Create the causal mask (lower triangle mask) for the encoder\n",
    "causal_mask = create_causal_mask(decoder_mask.shape[-1])\n",
    "\n",
    "# Combine the decoder mask and the causal mask\n",
    "causal_mask = causal_mask * decoder_mask\n",
    "\n",
    "# Plot all masks\n",
    "util.plot_boolean_masks(causal_mask, encoder_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5664a5816e3591f1",
   "metadata": {},
   "source": [
    "Note: For those who are wondering why the shape dont match up: The decoder mask and encoder mask dont have to be the same length for each sentence ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99655b8d79e74dd8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 12: Implement</h3>\n",
    "    <p>Add the padding masks at the appropriate spots in the code! Please go over the <code>forward()</code> passes in the <code>EncoderBlock</code>, <code>DecoderBlock</code> as well as the <code>Transformer</code> class in their respective files!\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b05c25b7454b945",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = test_task_12()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d54a9d",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    " We employ two types of regularization during training:\n",
    " \n",
    "- Residual Dropout: \n",
    "We apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of $P_{drop}$ = 0.1.\n",
    " \n",
    "- Label Smoothing: \n",
    "During training, we employed label smoothing of value $\\epsilon_{ls}$ = 0.1. This hurts perplexity, as the model learns to be more unsure, but improves accuracy. This means, that instead of using labels with one-hot encoding:\n",
    "\n",
    "$ y_{1hot} = \\begin{bmatrix}0 & 0 & 0 & \\cdots & 1 & \\cdots & 0 & 0 \\end{bmatrix} $\n",
    "\n",
    "Instead of zeros we use a small value $s = \\frac{\\epsilon_{ls}}{n_{cls} - 1}$, where $n_{cls}$ is the number of classes (=vocab_size). Since this has to be a proper distribution this has to add up 1. This results in a probability of being the correct word $p = 1 - \\epsilon_{ls}$\n",
    "\n",
    "For $n_{cls} = 11$ and $\\epsilon_ls = 0.1$, this would result in:\n",
    "\n",
    "$ y_{smooth} = \\begin{bmatrix}0.01 & 0.01 & 0.01 & 0.01 & 0.9 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 \\end{bmatrix} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137f4d18d8abf0ef",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 13: Implement</h3>\n",
    "    <p>Initialize dropout in the classes <code>Embedding</code>, <code>ScaledDotAttention</code>, <code>MultiHeadAttention</code> and <code>FeedForwardNeuralNetwork</code> in their respective files. Don't forget to add it in the <code>forrward()</code> pass! \n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5428624bc79f038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you get an Error about SCORE_SAVER - please just restart your kernel!\n",
    "_ = test_task_13()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96bf61fd0713dea",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 14: Check Code</h3>\n",
    "    <p>Have a look at <code>SmoothCrossEntropy</code> in <code>exercise_code/network/loss.py</code>\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03cdca13956f142",
   "metadata": {},
   "source": [
    "# Optimizer\n",
    "\n",
    "The paper used the Adam optimizer with $\\beta_1 = 0.9$, $\\beta_2 = 0.98$ and $\\epsilon = 10^{-9}$. </br>\n",
    "They varied the learning rate over the course of training, according to the formula: \n",
    "\n",
    "$lrate = d_{model}^{-0.5} \\cdot min(step\\_num^{0.5}, step\\_num \\cdot warmup\\_steps^{1.5})$\n",
    "\n",
    "This corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup_steps = 4000."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae400325ec6e7e1e",
   "metadata": {},
   "source": [
    "The setup could look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c184f04774015836",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from exercise_code.data.tokenizer import load_pretrained_fast\n",
    "\n",
    "tokenizer = load_pretrained_fast()\n",
    "model = Transformer(vocab_size=len(tokenizer), eos_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "d_model = model.d_model\n",
    "lr_start =d_model**-0.5\n",
    "eps=1e-9\n",
    "betas=(0.9, 0.98)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=lr_start, eps=eps, betas=betas)\n",
    "\n",
    "warm_up = 4000\n",
    "lr_lambda=lambda step: min((step+1)**-0.5, (step+1)*warm_up**-1.5)\n",
    "scheduler_example = LambdaLR(optimizer, lr_lambda=lr_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaf3e53da9001a1",
   "metadata": {},
   "source": [
    "Let's have a look at this scheduler function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810428a45c4d7b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = np.arange(0, 100000)\n",
    "lr = lr_start * np.vectorize(lr_lambda)(steps)\n",
    "\n",
    "plt.plot(steps, lr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b392f53e1d796c23",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Like already mentioned - Training is really where Transformers show there advantages compared to other sequential models! We can use a technique called **Teacher Forcing**, which means we feed the model the correct sentence, and the model only has to predict the next word! This is super easy to do in Transformers due to the causal masks we discussed earlier! In fact, all we have to do is pass the entire correct sentence into the decoder, and let it run exactly one time. The masking will make sure, each predicted word only depends on the previous words - no need to loop!\n",
    "\n",
    "For this exercise, we will not train a large model on a huge dataset, this would simply take too long and also isn't the focus of this exercise! However, you will have to implement the parts of the Trainer class, particular the forward pass! Note that have implemented a couple of extra functionalities, such as training from a checkpoint and a gradient accumulation. Gradient accumulation is used, to decouple the batch size from the optimizing step. Instead of performing an update after every batch, we can update our network after n batches. That way, we can choose a smaller batch size to save RAM, and still have the same effects as we would have with a larger batch! Be aware, that this doesn't work as nicely with batch normalization!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c47b90e4ad0b77",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 15: Implement</h3>\n",
    "    <p>Implement the <code>_forward()</code> method in the Trainer class in <code>exercise_code/trainer.py</code>\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64baecd09e2e9cf",
   "metadata": {},
   "source": [
    "Now we will overfit a small model on a dummy Dataset with small model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635e404efc259d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.trainer import Trainer\n",
    "from exercise_code.network import SmoothCrossEntropyLoss\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = None\n",
    "\n",
    "########################################################################\n",
    "# TODO:                                                                #\n",
    "#   Initialize your tokenizer. You train your own tokenizer and load   #\n",
    "#   load it like we did in the first notebook, or load the pretrained  #\n",
    "#   version!                                                           #\n",
    "#                                                                      #\n",
    "# Hint: Scroll up a couple of cells for the default tokenizer          #\n",
    "########################################################################\n",
    "\n",
    "\n",
    "pass\n",
    "\n",
    "########################################################################\n",
    "#                           END OF YOUR CODE                           #\n",
    "########################################################################\n",
    "\n",
    "hparams = None\n",
    "\n",
    "########################################################################\n",
    "# TODO:                                                                #\n",
    "#   Implement you model here                                           #\n",
    "#                                                                      #\n",
    "########################################################################\n",
    "\n",
    "\n",
    "pass\n",
    "\n",
    "########################################################################\n",
    "#                           END OF YOUR CODE                           #\n",
    "########################################################################\n",
    "model = Transformer(vocab_size=len(tokenizer), \n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                    hparams=hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbd36935f75405a",
   "metadata": {},
   "source": [
    "Alright, let's check the model size! For this task, the model should have less than 5 million parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debc89d5eb4b5b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = test_model_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8228df541ae40e7a",
   "metadata": {},
   "source": [
    "Alright, now we can define the dataset and the dataloader. The dataset only contains 1000 lines. We will also initialize the Trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b878608289c10f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = None\n",
    "scheduler = None\n",
    "\n",
    "epochs = None\n",
    "batch_size = None\n",
    "\n",
    "########################################################################\n",
    "# TODO:                                                                #\n",
    "#   Define the optimizer and optionally scheduler - not really needed  #\n",
    "#                                                                      #\n",
    "########################################################################\n",
    "\n",
    "\n",
    "pass\n",
    "\n",
    "########################################################################\n",
    "#                           END OF YOUR CODE                           #\n",
    "########################################################################\n",
    "\n",
    "loss_func = SmoothCrossEntropyLoss(smoothing=0.1)\n",
    "file_path = os.path.join(dataset_path, 'dummyDatasets', 'ds_dummy')\n",
    "collator = CustomCollator(tokenizer=tokenizer)\n",
    "dataset = CustomIterableDataset(file_path)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=collator)\n",
    "\n",
    "# For Apple M1/M2/M3 users: Try out the MPS framework, it will significantly speed up your training!\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif MPS_AVAILABLE:\n",
    "    if torch.backends.mps.is_available(): \n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  loss_func=loss_func,\n",
    "                  train_loader=dataloader,\n",
    "                  val_loader=None,\n",
    "                  optimizer=optimizer,\n",
    "                  scheduler=scheduler,\n",
    "                  epochs=epochs,\n",
    "                  device=device,\n",
    "                  optimizer_interval=0, # If you want to enable gradient accumulation, you can set this parameter! \n",
    "                  checkpoint_interval=0) # If you want to store your progress. You can resume training using train_from_checkpoint(#folder_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de8dd099294456d",
   "metadata": {},
   "source": [
    "Now let's finally train the model! To pass this task, you will need at least 50% accuracy on the dataset! Note: Don't be surprised if this will take a few epochs (>50 probably) and the accuracy will get better very slowly, especially in the beginning! Just let it run, we are overfitting on purpose so it should usually always work at some point!\n",
    "\n",
    "To explain the metrics: The first number is always the loss / accuracy over the current batch and the second number is always computed over the entire epoch.\n",
    "You can also resume training by just executing the cell again. If you reached the end of your epochs and run it again, it will also continue, starting where you left of for as many epochs as you configured. If you stop this cell, your models parameters will not be altered - in other words if you see you have reached the accuracy, just stop the cell ;) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974182aeee4cda2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99929169663da66",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = test_and_save_model(trainer, tokenizer, submission_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98a96e0dde07757",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "The last part to do is test your model in inference! We will feed an input sentence into the algorithm, together with a maximum number of iterations. The sentence is then tokenized, so that we can feed it into the model. The first decoder input will only be the start token, which in our model is the same as the end token! The model will give us an output distribution over all tokens in our vocabulary. We can then either choose the token with the highest probability, or we can sample from this categorical distribution! That way, every answer will be slightly different! The last output is added to the decoder sequence and on it goes! If the model predicts an end token, we stop the algorithm. Otherwise we continue until the maximum number of iterations are reached! Finally, we can decode the output. This will look something like this:\n",
    "\n",
    "\n",
    "Input Sentence: \"Hi how are you\" -> Tokenizer -> Encoder Input: [0, 45, 25, 15, 12, 0]\n",
    "\n",
    "| Iteration  | Encoder Input          | Decoder Input                | Decoder Output                                   |\n",
    "|------------|------------------------|------------------------------|--------------------------------------------------|\n",
    "| 1          | [0, 45, 25, 15, 12, 0] | [0]                          | [445]                                            |\n",
    "| 2          | [0, 45, 25, 15, 12, 0] | [0, 445]                     | [445, 56]                                        |\n",
    "| 3          | [0, 45, 25, 15, 12, 0] | [0, 445, 56]                 | [445, 56, 89]                                    |\n",
    "| 4          | [0, 45, 25, 15, 12, 0] | [0, 445, 56, 89]             | [445, 56, 89, 76]                                |\n",
    "| 5          | [0, 45, 25, 15, 12, 0] | [0, 445, 56, 89, 76]         | [445, 56, 89, 76, 0]                             | -> 0 Detected!\n",
    "\n",
    "Decoder Output: [445, 56, 89, 76, 0] -> Tokenizer -> Output Sentence: \"Hallo wie geht's dir <[EOS]>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e969f38",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 16: Check Code</h3>\n",
    "    <p>Have a look at the <code>predict()</code> method <code>Transformer</code> in <code>exercise_code/network/transformer.py</code>\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34fcc24c4a3ee07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(input_sentence, max_iteration_length = 50, probabilistic = False, returns_scores = False):\n",
    "    # Tokenize input sentence\n",
    "    encoder_input = torch.tensor(tokenizer.encode(input_sentence))\n",
    "\n",
    "    # Retrieve output sequence from model\n",
    "    output_sequence, score_records = model.predict(encoder_input, max_iteration_length, probabilistic, returns_scores)\n",
    "\n",
    "    # Decode output sequence\n",
    "    output_sequence = tokenizer.decode(output_sequence, skip_special_tokens=True)\n",
    "    \n",
    "    if returns_scores:\n",
    "        return output_sequence, score_records\n",
    "    return output_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9856c69d65cc8f",
   "metadata": {},
   "source": [
    "Now you can test your model! Feel free to change the variable probabilistic to true! don't be to surprised, if the output is terrible at the moment though!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff81d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just make sure the model is on the correct device\n",
    "# Usually the Trainer does this, just in case you stopped training mid epoch!\n",
    "_ = model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11848183f60a3b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_sequence = translate(\"Hi, how are you today?\", probabilistic=False)\n",
    "print(output_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3756c73bdefeaab",
   "metadata": {},
   "source": [
    "Awesome, it works! \n",
    "\n",
    "Well, sort of - This sentece was part of the dataset! Let's try it with a different sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3782e681",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_sequence = translate(\"This is not part of the dataset!\", probabilistic=False)\n",
    "print(output_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781aac04",
   "metadata": {},
   "source": [
    "Well.. ups :D But remember, all we did is overfit to a small dataset with a small model!\n",
    "\n",
    "We have prepared a pretrained model for you! It was trained on a larger dataset and has a lot more parameters! You should be able to load it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91548cb2c185c862",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = load_pretrained_fast()\n",
    "\n",
    "file_path = os.path.join(pretrained_model_path, 'pretrained_model')\n",
    "load_dict = torch.load(file_path)\n",
    "model = Transformer(vocab_size=len(tokenizer),\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                    hparams=load_dict['hparams'])\n",
    "\n",
    "model.load_state_dict(load_dict['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f397a48edee3e416",
   "metadata": {},
   "source": [
    "Now try it again and see how it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7342c760c205ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_sequence = translate(\"This is not part of the dataset!\", probabilistic=False)\n",
    "print(output_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fa9e302f00b65c",
   "metadata": {},
   "source": [
    "Alright, seems to work (Apart from a small typo, but we didn't train this model for that long)! Try out probabilistic to see some other results, they might be rubish though...\n",
    "\n",
    "Note: If your model outputs something weird, you probably made some mistake a long the way that we didn't catch! Please go over your Encoder and Decoder Blocks and make sure you did this correctly! Especially look at the residual connections!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316c4a623d4aa7a8",
   "metadata": {},
   "source": [
    "Congrats! You've now finished your first transformer model! Since this is a totally new exercise, we would really appreciate it if you could give us some [feedback](https://docs.google.com/forms/d/e/1FAIpQLSdVOrwS6PdP4pLnjtp0YmPXQFLinyP6mxNNcRXTQGng1hwGEA/viewform?usp=sf_link)! Like which explanations did you like or not like, what was to hard and maybe what was to easy! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dbbe05",
   "metadata": {},
   "source": [
    "To create a zip file with your submission, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ff7d9c337e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.util.submit import submit_exercise\n",
    "\n",
    "path = os.path.join(root_path, 'output', 'exercise11')\n",
    "submit_exercise(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7442471dcd3e8742",
   "metadata": {},
   "source": [
    "# Submission Instructions\n",
    "\n",
    "To complete the exercise, submit your final model to our submission portal - you probably know the procedure by now.\n",
    "\n",
    "1. Go on [our submission page](https://i2dl.vc.in.tum.de/submission/), register for an account and login. We use your matriculation number and send an email with the login details to the mail account associated. When in doubt, login into tum online and check your mails there. You will get an ID which we need in the next step.\n",
    "2. Log into [our submission page](https://i2dl.vc.in.tum.de/submission/) with your account details and upload the `zip` file. Once successfully uploaded, you should be able to see the submitted file selectable on the top.\n",
    "3. Click on this file and run the submission script. You will get an email with your score as well as a message if you have surpassed the threshold.\n",
    "\n",
    "# Submission Goals\n",
    "\n",
    "- Goal: Successfully implement a transformer model!\n",
    "\n",
    "- Points:\n",
    "    - 5 points per Module if shape is correct (Embedding, ScaledDotAttention, MultiHeadAttention, FeedForwardNeuralNetwork, EncoderBlock, DecoderBlock, Transformer)\n",
    "    - 5 points per Module if output is correct (Embedding, ScaledDotAttention, MultiHeadAttention, FeedForwardNeuralNetwork, EncoderBlock, DecoderBlock, Transformer)\n",
    "    - 30 points if submitted model reaches minimum score\n",
    "    - Total = 7 x 5 + 7 x 5 + 30 = 100\n",
    "\n",
    "- Passing Criteria: Minimum of 60 points!\n",
    "- Feel free to submit an unlimited number of assignments until the end of the semester; however, any submissions made after the deadline will not contribute to your bonus points.\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
